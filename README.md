# Vue Ollama Chat

[ä¸­æ–‡](./README_CN.md) | **English**

A modern, responsive chat application built with Vue 3, TypeScript, and Vite that integrates with Ollama for local AI model interactions. Now available as a cross-platform desktop application powered by Tauri.

## âœ¨ Features

- ğŸ¤– **Local AI Integration**: Connect to your local Ollama server for private AI conversations
- ğŸ’¬ **Real-time Streaming**: Experience smooth, real-time message streaming
- ğŸ¨ **Modern UI**: Beautiful, responsive interface built with Tailwind CSS and Reka UI
- ğŸŒ™ **Theme Support**: Light, dark, and system theme modes
- ğŸ’¾ **Persistent Storage**: Conversations and settings are automatically saved
- âš™ï¸ **Configurable**: Customize model parameters, system prompts, and server settings
- ğŸ“± **Responsive Design**: Works seamlessly on desktop and mobile devices
- ğŸ”§ **TypeScript**: Full type safety and excellent developer experience

## ğŸ–¥ï¸ Desktop Application Features

- ğŸ–¼ï¸ **Native Desktop Experience**: Cross-platform desktop app built with Tauri
- ğŸ”” **System Tray**: Minimize to system tray for background operation
- âš¡ **High Performance**: Rust backend + Vue frontend with minimal memory footprint
- ğŸ”’ **Security**: Sandboxed environment with local data processing
- ğŸ“¦ **Small Bundle Size**: Significantly smaller than Electron alternatives
- ğŸŒ **Cross-Platform**: Windows, macOS, and Linux support

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ 
- pnpm (recommended) or npm
- [Ollama](https://ollama.ai/) installed and running locally
- Rust 1.77.2+ (for desktop app development)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd vue-ollama-chat
```

2. Install dependencies:
```bash
pnpm install
```

3. Start the development server:

**Web Development:**
```bash
pnpm dev
```

**Desktop App Development:**
```bash
pnpm tauri:dev
```

4. For web version, open your browser and navigate to `http://localhost:5173`

### Ollama Setup

1. Install Ollama from [https://ollama.ai/](https://ollama.ai/)
2. Start Ollama service (usually runs on `http://localhost:11434`)
3. Pull a model (e.g., `ollama pull llama2`)
4. Configure the application settings to connect to your Ollama server

## ğŸ“¦ Building and Distribution

### Development Mode
```bash
# Start Tauri development mode
pnpm tauri:dev
```

### Build Desktop Application
```bash
# Build production desktop app
pnpm tauri:build
```

After building, executable files and installers are located at:
- **Windows**: `src-tauri/target/release/bundle/msi/` (MSI installer)
- **Windows**: `src-tauri/target/release/bundle/nsis/` (NSIS installer)
- **macOS**: `src-tauri/target/release/bundle/dmg/` (DMG file)
- **Linux**: `src-tauri/target/release/bundle/deb/` (DEB package)

### System Requirements

**Development Environment:**
- Node.js 18+
- Rust 1.77.2+
- pnpm (recommended)

**Runtime Environment:**
- Windows 10+ / macOS 10.15+ / Linux (mainstream distributions)
- [Ollama](https://ollama.ai/) local service

## ğŸ› ï¸ Tech Stack

- **Frontend Framework**: Vue 3 with Composition API
- **Language**: TypeScript
- **Build Tool**: Vite
- **Desktop Framework**: Tauri (Rust + WebView)
- **Styling**: Tailwind CSS
- **UI Components**: Reka UI
- **State Management**: Pinia with persistence
- **Routing**: Vue Router
- **Icons**: Lucide Vue Next
- **Markdown**: markdown-it with syntax highlighting
- **Notifications**: Vue Sonner
- **System Integration**: Native system tray, window management

## ğŸ“ Project Structure

src/
â”œâ”€â”€ components/          # Reusable UI components
â”‚   â”œâ”€â”€ ui/             # Base UI components
â”‚   â””â”€â”€ custom-error.vue # Error handling component
â”œâ”€â”€ composables/        # Vue composables
â”œâ”€â”€ services/           # API services (Ollama client)
â”œâ”€â”€ stores/             # Pinia stores
â”œâ”€â”€ types/              # TypeScript type definitions
â”œâ”€â”€ utils/              # Utility functions
â””â”€â”€ views/              # Page components
â”œâ”€â”€ chat/           # Chat interface
â”œâ”€â”€ setting/        # Settings page
â””â”€â”€ sider/          # Sidebar component


## âš™ï¸ Configuration

The application can be configured through the settings panel:

- **Ollama Server URL**: Default is `http://localhost:11434`
- **Model Selection**: Choose from available Ollama models
- **Temperature**: Control response creativity (0.0 - 2.0)
- **Max Tokens**: Set maximum response length
- **System Prompt**: Customize AI behavior
- **Theme**: Light, dark, or system preference

## ğŸ”§ Available Scripts

```bash
# Development
pnpm dev          # Start development server
pnpm build        # Build for production
pnpm preview      # Preview production build

# Code Quality
pnpm lint:eslint  # Lint TypeScript and Vue files
pnpm lint:ui      # Lint UI components
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for providing the local AI model infrastructure
- [Vue.js](https://vuejs.org/) team for the amazing framework
- [Reka UI](https://reka-ui.com/) for the beautiful UI components
- [Tailwind CSS](https://tailwindcss.com/) for the utility-first CSS framework